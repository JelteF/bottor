\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{float}
\newcommand{\BigO}[1]{\ensuremath{\operatorname{O}\bigl(#1\bigr)}}

\title{\textbf{Gedistribueerde Matrixvermenigvuldiging}\\
                Project Netcentric Computing}
\author{Jelte Fennema (10183159)\\
		Jaap Koetsier (10440615)\\
        Abe Wiersma (10433120)}
\date{26 juni 2014}

\begin{document}

\maketitle

\section{Inleiding}
Voor het project Netcentric Computing willen wij een systeem bouwen dat een zware
berekening distribueert over een willekeurig aantal clients. Deze berekening zal een
matrixvermenigvuldiging worden, omdat een matrixvermenigvuldiging eenvoudig willekeurig
groot te maken is en we hierdoor kunnen experimenteren met verschillende workloads. Ook
kunnen we eenvoudig testen met kleine berekeningen, voordat we gaan werken met grote
workloads. Nieuwe vermenigvuldigingen moeten toegevoegd en opgestart kunnen worden aan
de serverside via een grafische userinterface. Clients moeten zich op willekeurige momenten
aan- en af- kunnen melden en de server moet hier rekening mee kunnen houden.\\
De applicatie aan de clientside moet op de achtergrond kunnen draaien en zal zich bij een
hoog CPU-gebruik door andere applicaties inhouden om belangrijkere taken voorrang te geven.

\section{Implementatie}
De server implementeren we op basis van het Flask Framework \cite{flask}, een webframework
geschreven in python. We kiezen hiervoor omdat we allemaal - in meer of mindere mate -
ervaring hebben met Flask en we onze applicatie via het HTTP protocol willen laten communiceren,
en wel om de volgende reden:
Onze eerste intentie was om een soort van botnet op te zetten. Na enig brainstormwerk werd dit
deze gedistribueerde matrixvermenigvuldiging, maar een raakvlak met een botnet is er nog steeds.
Een botnet distribueert software dat op een willekeurig groot aantal computer uitgevoerd wordt,
wat wij ook gaan doen. Een botnet communiceerde voorheen namelijk vooral via het
HTTP protocol,
tegenwoordig wordt er ook van het IRC protocol gebruik gemaakt \cite{rajab06}. De reden
hiervoor is het feit dat HTTP en IRC nauwelijks geblokkeerd worden door de firewall, waardoor
de communicatie vrije doorgang heeft. Door ook het HTTP protocol te gebruiken hebben we geen
last van firewalls en kunnen we een HTTP API opzetten, waar Flask uitermate geschikt voor is.\\
\\
Aan de serverside wordt het meeste werk verricht. Via de grafische userinterface kunnen
matrices als tekstbestanden ge\"{u}pload worden. Vervolgens kan er van twee matrices een
job (matrixvermenigvuldiging) aangemaakt worden. Er wordt gecheckt of de vermenigvuldiging
mogelijk is (het aantal kolommen van matrix A moet gelijk zijn aan het aantal rijen van matrix
B) en van matrix B wordt de transpose gegenereerd. Vervolgens wordt de job in de queue gezet.
De transpose wordt gegenereerd om het feit dat het makkelijker is rijen uit een 2-dimensionale
array te halen dan kolommen. Daarnaast hoeven we maar \'{e}\'{e}n set functies te schrijven
voor bewerkingen met de bronmatrices.\\
Wanneer een client zich aanmeldt door middel van een handshake en een taskrequest stuurt maakt
de server een task aan. Een task is een deelberekening van de totale matrixvermenigvuldiging.
Voor het opsplitsen van taken hebben we gekozen voor de \emph{Block Cyclic Data Distribution}
methode \cite[2.2]{choi97}. Dit om zijn eenvoud: de server stuurt simpelweg een aantal rijen
van matrix A en een aantal kolommen van matrix B op naar de client. De client berekent met
deze submatrices A en B een submatrix C uit die hij terugstuurt naar de server. De server
voegt alle teruggekregen submatrices C samen tot het eindresultaat. Bij het opsturen van de
submatrices van A en B probeert de server zoveel mogelijk het aantal rijen van matrix A en het
aantal kolommen van B dat hij opstuurt gelijk te houden. Dit resulteert in een vierkante
submatrix als antwoord en levert de meeste resultaten op bij het minst te versturen data (zie
tabel 1).

\begin{table}[H]
    \begin{tabular}{|ll|lll}
    \hline
    Rijen matrix A & x & Kolommen matrix B & = & Cellen matrix C \\ \hline
    1              & x & 5                 & = & 5               \\
    2              & x & 4                 & = & 8               \\
    3              & x & 3                 & = & 9               \\ \hline
    \end{tabular}
    \caption{Overzicht aantal cellen in resultaatmatrix C bij het versturen van een totaal van 6
    rijen en kolommen. Wanneer het aantal rijen van A en het kolommen van B gelijk gehouden wordt
    zal het meeste resultaat ontvangen worden.}
\end{table}

Behalve het verdelen van taken houdt de server de administratie bij. De server houdt bij hoeveel
cellen van de resultaatmatrix C er al berekend zijn, hoeveel cellen er momenteel door clients
uitgerekend worden en hoeveel cellen er nog toegewezen moeten worden. Wanneer een client een
taak aanvraagt en de eerste job in de queue heeft geen niet-toegewezen cellen meer vrij, dan
wordt automatisch een taak toegewezen uit de volgende job indien beschikbaar. Als er geen taak
meer toegewezen kan worden krijgt de client hier bericht van en gaat de client in idle-stand.
In idle-stand probeert de client elke 5 seconden opnieuw een taak op te vragen, net zo lang tot
er weer een taak beschikbaar is.\\
Elke keer als een client een taak aanvraagt bij de server controleert de server eerst of er geen
taken zijn die verlopen zijn: dit zijn toegewezen taken waarvan de client niet binnen een vooraf
gestelde tijd (zie Constants.py) reageert en waarvan aangenomen kan worden dat dit resultaat niet
meer in redelijke tijd terugkomt. Deze taak wordt dan geannuleerd en kan opnieuw aan een andere
client worden toegewezen. Zo wordt voorkomen dat er onnodig lang gewacht wordt op clients die
het resultaat niet terug (kunnen) sturen.\\
De server houdt behalve de jobstatus ook de CPU-load van de clients bij. Clients sturen elke
500ms een ping naar de server waarmee ze laten blijken dat ze nog online zijn, plus hun CPU-load.

\section{Bevindingen}
Gedurende de implementatie liepen we tegen verbeterpunten aan die we on-the-fly verbeterd hebben.
In het begin leek het nut van de distributie van onze matrixvermenigvuldiging in het water te
vallen, omdat de taskrequests plus bijbehorende communicatie tussen client en server zodanig
veel overhead veroorzaakten dan er van de totale tijd weinig effectieve rekentijd was aan de
clientside: de client stond tussen de 80\% en 90\% van de tijd te wachten op reactie van de
server bij een taskrequest of een verzending van een resultaat.\\
\\
E\'{e}n van de eerste aanpassingen die we maakten om de relatieve overhead te verminderen was
het versturen van een groter aantal rijen en kolommen per taskrequest. In de testfase stond de
TASK\_SIZE (het aantal rijen en kolommen dat per taak toegewezen wordt) laag (op 3 tot 5) om het
overzicht te houden. Bij het versturen van 3 rijen en 3 kolommen komt er een resultaat van 9 cellen
terug ($3\times3$), dat is 1.5 cel per rij/kolom. De overhead per request gaat theoretisch lineair omhoog
($r + k$), maar het werkt dat de client met die data kan uitvoeren gaat kwadratisch omhoog ($r * k$).
Dit betekent dat bij het versturen van het dubbele aantal rijen en kolommen ($3 + 3$ of $6 + 6$), het
aantal resultaatcellen dat terugkomt omhoog gaat van 9 ($3 * 3$) naar 36 ($6 * 6$). Dat is al een
verdubbeling van het aantal resultaatcellen per rij/kolom (van 1.5 naar 3, $9/6$ en $36/12$). Wij
hebben de TASK\_SIZE verhoogd naar 200, wat $(200*200)/(200+200) = 100$ resultaatcellen per
opgestuurde rij/kolom oplevert. Hierdoor komen de verhoudingen requesttijd/rekentijd beduidend
gunstiger te liggen.\\
De tweede verbetering die we aangebracht hebben is het vermijden van het lezen en schrijven
naar en van bestanden bij het lezen of aanpassen van een matrix. Dit wisten we eigenlijk vooraf
al, maar omdat de basis van de server snel moest staan is dit bij de opzet genegeerd. In de
eindapplicatie worden de matrices in geheugen geladen als arrays, zodat de bronmatrices beiden
eenmalig ingelezen hoeven te worden, en de resultaatmatrix slechts eenmaal weggeschreven hoeft
te worden (bij voltooiing van de job). De precieze tijd dat hiermee gewonnen wordt is sterk
afhankelijk van de grootte van de matrix, dus precieze cijfers zijn moeilijk te geven, maar bij
matrixgroottes vanaf $250\times250$ is de winst al duidelijk merkbaar.

De laatste grote verbetering zat in de JSON serialisatie. Deze overhead is op twee manieren
aangepast. Allereerst is de structuur van de API aangepast. In plaats van het sturen van rijen
en kolommen als losse arrays, stuurt de server nu twee 2-dimensionale arrays A en B,
bestaande respectievelijk uit de rijen van matrix A en de kolommen van matrix B die de client
met elkaar moet vermenigvuldigen. Dit gaf al enige snelheidswinst, maar er moest hier meer winst
te behalen zijn. Uit timings bij de serialisatie bleek dat bij het serialiseren van alle floats
te zijn. Door enige aanpassingen along-the-way in de code was er een onnodig grote overhead
ontstaan: de matrices worden ingelezen uit een tekstbestand als strings, waarna ze in een array
gezet worden als floating point getallen. Om de entries van de matrix vervolgens op te sturen
naar de client worden ze omgezet in JSON-formaat. JSON verstuurd de net omgezette floating points
echter als strings en converteert de floating point getallen weer. Aan de client-side worden de
strings vervolgens weer omgezet naar floating point getallen. Aan de server-side zijn floating
point getallen niet nodig, dus hier hebben we de bronmatrices uiteindelijk gewoon als strings
laten staan.

Dit alles heeft ervoor gezorgd dat, natuurlijk afhankelijk van de server en de
client, er ongeveer 3 seconden nodig is per request aan overhead en zo'n 12 aan
werk voor de client bij een matrix van $1000\times1000$. Als er meerdere clients
verbonden zijn kan die overhead flink groeien doordat de requests dan tegelijk
aan kunnen komen bij de server en een client dan ook moet wachten op de overhead
van de request voor de eerdere client(s). Dit overhead probleem laat al snel van
zich merken bij het gebruik. Met 2 en 3 clients is er duidelijk een speedup
aanwezig, maar vanaf 4 verschillende clients wordt de overhead al de bottleneck.
Dit probleem kan ook niet verminderd worden door grotere matrices te nemen.
Omdat je dan meer data per rij en kolom moet versturen wordt de verhouding
tussen de hoeveelheid te versturen data en het aantal berekingen dat met de data
gedaan kan worden minder juist ongunstiger.


\section{Conclusie}
Het distribueren van lastige reken taken met een protocol lijkend op het protocol
wat wij gebruikt hebben is goed mogelijk. Het is een goede manier om te zorgen
dat het zwaar werk verdeeld kan worden over normale computers. Wat alleen erg
belangrijk is om rekening mee te houden, is de verhouding rekentijd en grote van
de dataset. De load van het versturen van de data kan voor de server grote
problemen opleveren. Dit wordt alleen maar erger als er meer clients mee helpen
en kan dit ervoor zorgen dat de speedup stagneert. Bij het vermenigvuldigen van
matrices is die data distributie overhead erg groot en stagneert de speedup
daarom snel.



\section{Discussie}
Er zijn twee oplossingen die verder onderzocht zouden kunnen worden om de
overhead op de server te verminderen. Allereerst zou men een moeilijker probleem
kunnen proberen op te lossen. Aangezien naive matrixvermenigvuldiging een
complexiteit heeft van $\BigO{n^3}$ zou bijvoorbeeld een probleem gezocht
kunnen worden met een complexiteit van $\BigO{2^n}$. Dan kan de client veel meer
werk met dezelfde hoeveelheid data.

Als andere oplossing zou gekeken kunnen worden naar het gedistribueerd maken van
de distributie van de data. Een mogelijke manier hiervoor zou kunnen zijn dat de
matrices opgesplitst worden in losse files en dat die daarna doormiddel van
peer-to-peer technologie (torrents) verspreid worden tussen de clients. Op die
manier hoeft de server in principe maar een paar keer de files te versturen. En
kan het daarna in theorie de verspreiding hiervan overgenomen worden door de
clients.


\begin{thebibliography}{9}

\bibitem{flask}
  http://flask.pocoo.org.
\bibitem{choi97}
  Jaeyoung Choi,
  \emph{A New Parallel Matrix Multiplication Algorithm on Distributed-Memory Concurrent Computers}.
  Soongsil University, Korea,
  1997.

\bibitem{rajab06}
  Rajab, Zarfoxx, Monrose, Terzis,
  \emph{A Multifaceted Approach to Understanding the Botnet Phenomenon}.
  Johns Hopkins University,
  2006.


\end{thebibliography}
\end{document}
