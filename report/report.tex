\documentclass[11pt]{article}

\usepackage{float}

\title{\textbf{Gedistribueerde Matrixvermenigvuldiging}\\
                Project Netcentric Computing}
\author{Jelte Fennema (10183159)\\
		Jaap Koetsier (10440615)\\
        Abe Wiersma (10433120)}
\date{26 juni 2014}

\begin{document}

\maketitle

\section{Inleiding}
Voor het project Netcentric Computing willen wij een systeem bouwen dat een zware
berekening distribueert over een willekeurig aantal clients. Deze berekening zal een
matrixvermenigvuldiging worden, omdat een matrixvermenigvuldiging eenvoudig willekeurig
groot te maken is en we hierdoor kunnen experimenteren met verschillende workloads. Ook
kunnen we eenvoudig testen met kleine berekeningen, voordat we gaan werken met grote
workloads. Nieuwe vermenigvuldigingen moeten toegevoegd en opgestart kunnen worden aan
de serverside via een grafische userinterface. Clients moeten zich op willekeurige momenten
aan- en af- kunnen melden en de server moet hier rekening mee kunnen houden.\\
De applicatie aan de clientside moet op de achtergrond kunnen draaien en zal zich bij een
hoog CPU-gebruik door andere applicaties inhouden om belangrijkere taken voorrang te geven.

\section{Implementatie}
De server implementeren we op basis van het Flask Framework \cite{flask}, een webframework
geschreven in python. We kiezen hiervoor omdat we allemaal - in meer of mindere mate -
ervaring hebben met Flask en we onze applicatie via het HTTP protocol willen laten communiceren,
en wel om de volgende reden:
Onze eerste intentie was om een soort van botnet op te zetten. Na enig brainstormwerk werd dit
deze gedistribueerde matrixvermenigvuldiging, maar een raakvlak met een botnet is er nog steeds.
Een botnet distribueert software dat op een willekeurig groot aantal computer uitgevoerd wordt,
wat wij ook gaan doen. Een botnet communiceerde voorheen namelijk vooral via het HTTP protocol
(tegenwoordig wordt er meer van het IRC protocol gebruik gemaakt) \cite{rajab06}. De reden
hiervoor is het feit dat HTTP en IRC nauwelijks geblokkeerd worden door de firewall, waardoor
de communicatie vrije doorgang heeft. Door ook het HTTP protocol te gebruiken hebben we geen
last van firewalls en kunnen we een HTTP API opzetten, waar Flask uitermate geschikt voor is.\\
\\
Aan de serverside wordt het meeste werk verricht. Via de grafische userinterface kunnen
matrices als tekstbestanden ge\"{u}pload worden. Vervolgens kan er van twee matrices een
job (matrixvermenigvuldiging) aangemaakt worden. Er wordt gecheckt of de vermenigvuldiging
mogelijk is (het aantal kolommen van matrix A moet gelijk zijn aan het aantal rijen van matrix
B) en van matrix B wordt de transpose gegenereerd. Vervolgens wordt de job in de queue gezet.
De transpose wordt gegenereerd om het feit dat het makkelijker is rijen uit een 2-dimensionale
array te halen dan kolommen. Daarnaast hoeven we maar \'{e}\'{e}n set functies te schrijven
voor bewerkingen met de bronmatrices.\\
Wanneer een client zich aanmeldt door middel van een handshake en een taskrequest stuurt maakt
de server een task aan. Een task is een deelberekening van de totale matrixvermenigvuldiging.
Voor het opsplitsen van taken hebben we gekozen voor de \emph{Block Cyclic Data Distribution}
methode \cite[2.2]{choi97}. Dit om zijn eenvoud: de server stuurt simpelweg een aantal rijen
van matrix A en een aantal kolommen van matrix B op naar de client. De client berekent met
deze submatrices A en B een submatrix C uit die hij terugstuurt naar de server. De server
voegt alle teruggekregen submatrices C samen tot het eindresultaat. Bij het opsturen van de
submatrices van A en B probeert de server zoveel mogelijk het aantal rijen van matrix A en het
aantal kolommen van B dat hij opstuurt gelijk te houden. Dit resulteert in een vierkante
submatrix als antwoord en levert de meeste resultaten op bij het minst te versturen data (zie
tabel 1).

\begin{table}[H]
    \begin{tabular}{|ll|lll}
    \hline
    Rijen matrix A & x & Kolommen matrix B & = & Cellen matrix C \\ \hline
    1              & x & 5                 & = & 5               \\
    2              & x & 4                 & = & 8               \\
    3              & x & 3                 & = & 9               \\ \hline
    \end{tabular}
    \caption{Overzicht aantal cellen in resultaatmatrix C bij het versturen van een totaal van 6 
    rijen en kolommen. Wanneer het aantal rijen van A en het kolommen van B gelijk gehouden wordt
    zal het meeste resultaat ontvangen worden.}
\end{table}

Behalve het verdelen van taken houdt de server de adminstratie bij. De server houdt bij hoeveel
cellen van de resultaatmatrix C er al berekend zijn, hoeveel cellen er momenteel door clients
uitgerekend worden en hoeveel cellen er nog toegewezen moeten worden. Wanneer een client een
task aanvraagt en de eerste job in de queue heeft geen niet-toegewezen cellen meer vrij, dan
wordt automatisch een taak toegewezen uit de volgende job indien beschikbaar. Als er geen taak
meer toegewezen kan worden krijgt de client hier bericht van en gaat de client in idle-stand.
In idle-stand probeert de client elke 5 seconden opnieuw een taak op te vragen, net zo lang tot
er weer een taak beschikbaar is.\\
Elke keer als een client een task aanvraagt bij de server controleert de server eerst of er geen
tasks zijn die verlopen zijn: dit zijn toegewezen tasks waarvan de client niet binnen een vooraf
gestelde tijd (zie Constants.py) reageert en waarvan aangenomen kan worden dat dit resultaat niet
meer in redelijke tijd terugkomt. Deze task wordt dan geannuleerd en kan opnieuw aan een andere
client worden toegewezen. Zo wordt voorkomen dat er onnodig lang gewacht wordt op clients die
het resultaat niet terug (kunnen) sturen.\\
De server houdt behalve de jobstatus ook de CPU-load van de clients bij. Clients sturen elke
500ms een ping naar de server waarmee ze laten blijken dat ze nog online zijn, plus hun CPU-load.

\section{Bevindingen}
Gedurende de implementatie liepen we tegen verbeterpunten aan die we on-the-fly verbeterd hebben.
In het begin leek het nut van de distributie van onze matrixvermenigvuldiging in het water te
vallen, omdat de taskrequests plus bijbehorende communicatie tussen client en server zodanig
veel overhead veroorzaakten dan er van de totale tijd weinig effectieve rekentijd was aan de 
clientside: de client stond tussen de 80\% en 90\% van de tijd te wachten op reactie van de
server bij een taskrequest of een verzending van een resultaat.\\
\\
E\'{e}n van de eerste aanpassingen die we maakten om de relatieve overhead te verminderen was
het versturen van een groter aantal rijen en kolommen per taskrequest. In de testfase stond de
TASK\_SIZE (het aantal rijen en kolommen dat per taak toegewezen wordt) laag (op 3 tot 5) om het
overzicht te houden. Bij het versturen van 3 rijen en 3 kolommen komt er een resultaat van 9 cellen
terug (3x3), dat is 1.5 cel per rij/kolom. De overhead per request gaat theoretisch lineair omhoog
($r + k$), maar het werkt dat de client met die data kan uitvoeren gaat kwadratisch omhoog ($r * k$).
Dit betekent dat bij het versturen van het dubbele aantal rijen en kolommen ($3 + 3$ of $6 + 6$), het
aantal resultaatcellen dat terugkomt omhoog gaat van 9 ($3 * 3$) naar 36 ($6 * 6$). Dat is al een
verdubbeling van het aantal resultaatcellen per rij/kolom (van 1.5 naar 3, $9/6$ en $36/12$). Wij
hebben de TASK\_SIZE verhoogd naar 200, wat $(200*200)/(200+200) = 100$ resultaatcellen per
opgestuurde rij/kolom oplevert. Hierdoor komen de verhoudingen requesttijd/rekentijd beduidend
gunstiger te liggen.\\
De tweede verbetering die we aangebracht hebben is het vermijden van het lezen en schrijven
naar en van bestanden bij het lezen of aanpassen van een matrix. Dit wisten we eigenlijk vooraf
al, maar omdat de basis van de server snel moest staan is dit bij de opzet genegeerd. In de
eindapplicatie worden de matrices in geheugen geladen als arrays, zodat de bronmatrices beiden
eenmalig ingelezen hoeven te worden, en de resultaatmatrix slechts eenmaal weggeschreven hoeft
te worden (bij voltooiing van de job). De precieze tijd dat hiermee gewonnen wordt is sterk
afhankelijk van de grootte van de matrix, dus precieze cijfers zijn moeilijk te geven, maar bij
matrixgroottes vanaf 250x250 is de winst al duidelijk merkbaar.

De laatste grote verbetering zat in de JSON serialisatie. Deze overhead is op twee manieren
aangepast. Allereerst is de structuur van de API aangepast. In plaats van het sturen van rijen
en kolommen als losse arrays, stuurt de server nu twee 2-dimensionale arrays A en B,
bestaande respectievelijk uit de rijen van matrix A en de kolommen van matrix B die de client
met elkaar moet vermenigvuldigen. Dit gaf al enige snelheidswinst, maar er moest hier meer winst
te behalen zijn. Uit timings bij de serialisatie bleek dat bij het serialiseren van alle floats
te zijn. Door enige aanpassingen along-the-way in de code was er een onnodig grote overhead
ontstaan: de matrices worden ingelezen uit een tekstbestand als strings, waarna ze in een array
gezet worden als floating point getallen. Om de entries van de matrix vervolgens op te sturen
naar de client worden ze omgezet in JSON-formaat. JSON verstuurd de net omgezette floating points
echter als strings en converteert de floating point getallen weer. Aan de client-side worden de
strings vervolgens weer omgezet naar floating point getallen. Aan de server-side zijn floating
point getallen niet nodig, dus hier hebben we de bronmatrices uiteindelijk gewoon als strings
laten staan.\\
Deze verbeteringen samen hebben ervoor gezorgd dat een taskrequest naar de server in een oogwenk
gedaan is en de effectieve rekentijd van de client boven de 95\% zit. Dit laatste is natuurlijk
erg afhankelijk van de afstand en verbinding tussen client en server dus kunnen niet als harde
cijfers beschouwd worden, maar we kunnen met aan zekerheid grenzende waarschijnlijkheid stellen
dat de overall performance fors verbeterd is.

\section{Conclusie}

\begin{thebibliography}{9}

\bibitem{flask}
  http://flask.pocoo.org.
\bibitem{choi97}
  Jaeyoung Choi,
  \emph{A New Parallel Matrix Multiplication Algorithm on Distributed-Memory Concurrent Computers}.
  Soongsil University, Korea,
  1997.
  
\bibitem{rajab06}
  Rajab, Zarfoxx, Monrose, Terzis,
  \emph{A Multifaceted Approach to Understanding the Botnet Phenomenon}.
  Johns Hopkins University,
  2006.
  

\end{thebibliography}
\end{document}
